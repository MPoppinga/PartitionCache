name: Integration Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

env:
  CI: true
  PYTHON_VERSION: "3.12"

jobs:
  build-postgres-image:
    name: Build PostgreSQL Image with Extensions
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
      actions: write
    outputs:
      image-tag: sha-${{ steps.sha.outputs.short }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Get short SHA
      id: sha
      run: echo "short=${GITHUB_SHA::7}" >> $GITHUB_OUTPUT

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to GitHub Container Registry
      uses: docker/login-action@v3
      with:
        registry: ghcr.io
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ghcr.io/mpoppinga/postgres-test-extensions
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix=sha-
          type=raw,value=latest,enable={{is_default_branch}}

    - name: Build and push PostgreSQL image
      uses: docker/build-push-action@v5
      with:
        context: .github/docker/postgres-cron
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha,scope=${{ github.workflow }}
        cache-to: type=gha,mode=max,scope=${{ github.workflow }}
        platforms: linux/amd64

  integration-tests:
    name: ${{ matrix.name }}
    runs-on: ubuntu-latest
    needs: build-postgres-image
    
    strategy:
      fail-fast: false
      matrix:
        include:
          - name: "PostgreSQL Array Backend"
            pattern: "postgresql_array"
            backend_filter: "postgresql_array"
            integration_backends: "postgresql_array"
            suffix: "pg_array"
            backend_type: "postgresql_array"
            database_suffix: "array"
            parallel: false
            requires_redis: false
            job_index: 0
          - name: "PostgreSQL Bit Backend"
            pattern: "postgresql_bit"
            backend_filter: "postgresql_bit"
            integration_backends: "postgresql_bit"
            suffix: "pg_bit"
            backend_type: "postgresql_bit"
            database_suffix: "bit"
            parallel: false
            requires_redis: false
            job_index: 1
          - name: "PostgreSQL RoaringBit Backend"
            pattern: "postgresql_roaringbit"
            backend_filter: "postgresql_roaringbit"
            integration_backends: "postgresql_roaringbit"
            suffix: "pg_roaring"
            backend_type: "postgresql_roaringbit"
            database_suffix: "roaring"
            parallel: false
            requires_redis: false
            job_index: 2
          - name: "Redis Set Backend"
            pattern: "redis_set"
            backend_filter: "redis_set"
            integration_backends: "redis_set"
            suffix: "redis_set"
            backend_type: "redis_set"
            database_suffix: "redis_set"
            parallel: false
            requires_redis: true
            job_index: 0
            redis_set_db: 0
          - name: "Redis Bit Backend"
            pattern: "redis_bit"
            backend_filter: "redis_bit"
            integration_backends: "redis_bit"
            suffix: "redis_bit"
            backend_type: "redis_bit"
            database_suffix: "redis_bit"
            parallel: false
            requires_redis: true
            job_index: 1
            redis_bit_db: 1
          - name: "RocksDB Backends"
            pattern: "rocksdb"
            backend_filter: "rocksdb_set or rocksdb_bit or rocksdict or rocksdict_roaringbit"
            integration_backends: "rocksdb_set,rocksdb_bit,rocksdict,rocksdict_roaringbit"
            suffix: "rocksdb"
            backend_type: "rocksdb"
            database_suffix: "rocksdb"
            parallel: false
            requires_redis: false
            job_index: 3
          - name: "DuckDB Acceleration"
            pattern: "duckdb_acceleration"
            backend_filter: "postgresql_array"
            integration_backends: "postgresql_array"
            suffix: "duckdb_accel"
            backend_type: "postgresql_array"
            database_suffix: "duckdb_accel"
            parallel: false
            requires_redis: false
            job_index: 4
          - name: "Queue Processing"
            pattern: "queue"
            backend_filter: "postgresql_array"
            integration_backends: "postgresql_array"
            suffix: "queue"
            backend_type: "postgresql_array"
            database_suffix: "queue"
            parallel: false
            requires_redis: false
            job_index: 5
          - name: "Spatial Cache"
            pattern: "spatial"
            backend_filter: "postgresql_array"
            integration_backends: "postgresql_array"
            suffix: "spatial"
            backend_type: "postgresql_array"
            database_suffix: "spatial"
            parallel: false
            requires_redis: false
            job_index: 6
          - name: "Spatial Cache (RoaringBit)"
            pattern: "spatial"
            backend_filter: "postgresql_roaringbit"
            integration_backends: "postgresql_roaringbit"
            suffix: "spatial_roaring"
            backend_type: "postgresql_roaringbit"
            database_suffix: "spatial_roaring"
            parallel: false
            requires_redis: false
            job_index: 9
          - name: "Maintenance Operations"
            pattern: "maintenance"
            backend_filter: "postgresql_array"
            integration_backends: "postgresql_array"
            suffix: "maintenance"
            backend_type: "postgresql_array"
            database_suffix: "maintenance"
            parallel: false
            requires_redis: false
            job_index: 7
          - name: "Pipeline Integration"
            pattern: "constraint_to_cache_pipeline"
            backend_filter: "postgresql_array"
            integration_backends: "postgresql_array"
            suffix: "pipeline"
            backend_type: "postgresql_array"
            database_suffix: "pipeline"
            parallel: false
            requires_redis: false
            job_index: 8

    services:
      postgres:
        image: ghcr.io/mpoppinga/postgres-test-extensions:${{ needs.build-postgres-image.outputs.image-tag }}
        env:
          POSTGRES_USER: integration_user
          POSTGRES_PASSWORD: integration_password
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U integration_user -d postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10
      
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10

    env:
      # Define a unique database name for each backend to ensure complete isolation
      UNIQUE_DB_NAME: partitioncache_${{ matrix.database_suffix }}_${{ github.run_id }}
      
      # Set the specific backend type for this job
      CACHE_BACKEND: ${{ matrix.backend_type }}
      INTEGRATION_CACHE_BACKENDS: ${{ matrix.integration_backends }}
      INTEGRATION_TEST_ENABLED: 1
      
      # PostgreSQL configuration
      PG_HOST: localhost
      PG_PORT: 5432
      PG_USER: integration_user
      PG_PASSWORD: integration_password
      
      # Database configuration
      DB_HOST: localhost
      DB_PORT: 5432
      DB_USER: integration_user
      DB_PASSWORD: integration_password
      
      # Queue configuration
      PG_QUEUE_HOST: localhost
      PG_QUEUE_PORT: 5432
      PG_QUEUE_USER: integration_user
      PG_QUEUE_PASSWORD: integration_password
      QUERY_QUEUE_PROVIDER: postgresql
      
      # Redis configuration - only set for Redis backends
      ${{ matrix.requires_redis && 'REDIS_HOST' || 'REDIS_HOST_DISABLED' }}: ${{ matrix.requires_redis && 'localhost' || 'disabled' }}
      ${{ matrix.requires_redis && 'REDIS_PORT' || 'REDIS_PORT_DISABLED' }}: ${{ matrix.requires_redis && '6379' || 'disabled' }}
      ${{ matrix.redis_set_db != null && 'REDIS_CACHE_DB' || 'REDIS_CACHE_DB_DISABLED' }}: ${{ matrix.redis_set_db || 'disabled' }}
      ${{ matrix.redis_set_db != null && 'REDIS_SET_DB' || 'REDIS_SET_DB_DISABLED' }}: ${{ matrix.redis_set_db || 'disabled' }}
      ${{ matrix.redis_bit_db != null && 'REDIS_BIT_DB' || 'REDIS_BIT_DB_DISABLED' }}: ${{ matrix.redis_bit_db || 'disabled' }}
      ${{ matrix.requires_redis && 'REDIS_BIT_BITSIZE' || 'REDIS_BIT_BITSIZE_DISABLED' }}: ${{ matrix.requires_redis && '300000' || 'disabled' }}
      ${{ matrix.requires_redis && 'REDIS_SET_HOST' || 'REDIS_SET_HOST_DISABLED' }}: ${{ matrix.requires_redis && 'localhost' || 'disabled' }}
      ${{ matrix.requires_redis && 'REDIS_SET_PORT' || 'REDIS_SET_PORT_DISABLED' }}: ${{ matrix.requires_redis && '6379' || 'disabled' }}
      ${{ matrix.requires_redis && 'REDIS_BIT_HOST' || 'REDIS_BIT_HOST_DISABLED' }}: ${{ matrix.requires_redis && 'localhost' || 'disabled' }}
      ${{ matrix.requires_redis && 'REDIS_BIT_PORT' || 'REDIS_BIT_PORT_DISABLED' }}: ${{ matrix.requires_redis && '6379' || 'disabled' }}
      
      # Cache backend configuration with backend-specific prefixes for complete isolation
      PG_ARRAY_CACHE_TABLE_PREFIX: ci_array_${{ matrix.database_suffix }}
      PG_BIT_CACHE_TABLE_PREFIX: ci_bit_${{ matrix.database_suffix }}
      PG_BIT_CACHE_BITSIZE: 300000
      PG_ROARINGBIT_CACHE_TABLE_PREFIX: ci_roaring_${{ matrix.database_suffix }}

      # PostGIS spatial cache configuration (used by spatial test job)
      PG_H3_SRID: 25832
      PG_BBOX_SRID: 25832
      PG_H3_CACHE_TABLE_PREFIX: ci_h3_${{ matrix.database_suffix }}
      PG_BBOX_CACHE_TABLE_PREFIX: ci_bbox_${{ matrix.database_suffix }}
      
      # Queue configuration with backend-specific prefixes
      PG_QUEUE_TABLE_PREFIX: ci_queue_${{ matrix.database_suffix }}
      
      # RocksDB configuration - isolated per job to prevent file conflicts
      ROCKSDB_PATH: /tmp/ci_rocksdb_${{ matrix.suffix }}_${{ github.run_id }}
      ROCKSDB_BIT_PATH: /tmp/ci_rocksdb_bit_${{ matrix.suffix }}_${{ github.run_id }}
      ROCKSDB_BIT_BITSIZE: 300000
      ROCKSDB_DICT_PATH: /tmp/ci_rocksdict_${{ matrix.suffix }}_${{ github.run_id }}

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup micromamba
      uses: mamba-org/setup-micromamba@v1
      with:
        micromamba-version: 'latest'
        environment-file: .github/micromamba-env.yml
        init-shell: >-
          bash
        cache-environment: false
        post-cleanup: 'all'

    - name: Install PartitionCache
      shell: micromamba-shell {0}
      run: |
        pip install -e ".[testing,db]"
        pip install pytest-xdist  # Additional for parallel execution

    - name: Create and configure test database
      shell: micromamba-shell {0}
      run: |
        echo "Waiting for PostgreSQL..."
        for i in {1..30}; do
          if python -c "import psycopg; psycopg.connect('postgresql://integration_user:integration_password@localhost:5432/postgres')" 2>/dev/null; then
            echo "PostgreSQL ready"
            break
          fi
          sleep 2
        done

        echo "Creating clean isolated test database: $UNIQUE_DB_NAME"
        python scripts/create_clean_test_database.py

        # Set the database name for subsequent steps
        echo "PG_DBNAME=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "DB_NAME=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "PG_QUEUE_DB=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "PG_CRON_DATABASE=$UNIQUE_DB_NAME" >> $GITHUB_ENV

        if [[ "${{ matrix.requires_redis }}" == "true" ]]; then
          echo "Waiting for Redis..."
          for i in {1..30}; do
            if redis-cli -h localhost -p 6379 ping >/dev/null 2>&1; then
              echo "Redis ready"
              break
            fi
            sleep 2
          done
        fi

    - name: Verify extensions
      shell: micromamba-shell {0}
      run: |
        python -c "
        import os
        import psycopg
        db_name = os.environ['PG_DBNAME']
        conn_str = f'postgresql://integration_user:integration_password@localhost:5432/{db_name}'
        with psycopg.connect(conn_str, autocommit=True) as conn:
            with conn.cursor() as cur:
                # Check for roaringbitmap extension (should be available in all test databases)
                cur.execute('SELECT extname FROM pg_extension WHERE extname = %s', ('roaringbitmap',))
                if cur.fetchone():
                    print('roaringbitmap extension verified')
                else:
                    print('roaringbitmap extension not found (warning)')
                
                # Check for pg_cron extension (only available in designated cron database)
                cur.execute('SELECT extname FROM pg_extension WHERE extname = %s', ('pg_cron',))
                if cur.fetchone():
                    print('pg_cron extension verified (cron-enabled database)')
                else:
                    print('pg_cron extension not found (expected for regular test databases)')

                # Check for PostGIS extension
                cur.execute('SELECT extname, extversion FROM pg_extension WHERE extname = %s', ('postgis',))
                row = cur.fetchone()
                if row:
                    print(f'PostGIS extension verified (version {row[1]})')
                else:
                    print('PostGIS extension not found (spatial tests will skip)')

                # Check for h3 extension
                cur.execute('SELECT extname, extversion FROM pg_extension WHERE extname = %s', ('h3',))
                row = cur.fetchone()
                if row:
                    print(f'h3 extension verified (version {row[1]})')
                else:
                    print('h3 extension not found (H3 spatial tests will skip)')

                # Check for pois table
                cur.execute(\"\"\"SELECT count(*) FROM information_schema.tables WHERE table_name = 'pois'\"\"\")
                row = cur.fetchone()
                if row and row[0] > 0:
                    cur.execute('SELECT count(*) FROM pois')
                    count = cur.fetchone()
                    print(f'pois table verified ({count[0]} rows)')
                else:
                    print('pois table not found (spatial tests will skip)')

                print(f'Database {db_name} extension verification completed')
        "

    - name: Run tests
      shell: micromamba-shell {0}
      run: |
        ./scripts/run_integration_tests.sh \
          --ci \
          --pattern "${{ matrix.pattern }}" \
          --backend "${{ matrix.backend_filter }}" \
          --verbose \
          --ignore tests/integration/test_pg_cron_integration.py \
          --no-setup

    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-${{ matrix.name }}
        path: |
          /tmp/ci_rocksdb*/
          /tmp/ci_rocksdict*/
        retention-days: 7
        if-no-files-found: ignore


  cli-tools-tests:
    name: CLI Tools Tests (pg_cron)
    runs-on: ubuntu-latest
    needs: [build-postgres-image, integration-tests]
    if: github.event_name == 'push' || github.event_name == 'pull_request'
    
    services:
      postgres:
        image: ghcr.io/mpoppinga/postgres-test-extensions:${{ needs.build-postgres-image.outputs.image-tag }}
        env:
          POSTGRES_USER: integration_user
          POSTGRES_PASSWORD: integration_password
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U integration_user -d postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10

    env:
      UNIQUE_DB_NAME: partitioncache_integration
      CACHE_BACKEND: postgresql_array
      PG_HOST: localhost
      PG_PORT: 5432
      PG_USER: integration_user
      PG_PASSWORD: integration_password
      DB_HOST: localhost
      DB_PORT: 5432
      DB_USER: integration_user
      DB_PASSWORD: integration_password
      PG_QUEUE_HOST: localhost
      PG_QUEUE_PORT: 5432
      PG_QUEUE_USER: integration_user
      PG_QUEUE_PASSWORD: integration_password
      QUERY_QUEUE_PROVIDER: postgresql
      PG_ARRAY_CACHE_TABLE_PREFIX: ci_array_cache_cli
      PG_QUEUE_TABLE_PREFIX: ci_queue_cli
      INTEGRATION_CACHE_BACKENDS: postgresql_array

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup micromamba
      uses: mamba-org/setup-micromamba@v1
      with:
        micromamba-version: 'latest'
        environment-file: .github/micromamba-env.yml
        init-shell: >-
          bash
        cache-environment: false
        post-cleanup: 'all'

    - name: Install PartitionCache
      shell: micromamba-shell {0}
      run: |
        pip install -e ".[testing,db]"

    - name: Verify and configure pg_cron database
      shell: micromamba-shell {0}
      run: |
        echo "Waiting for PostgreSQL..."
        for i in {1..30}; do
          if python -c "import psycopg; psycopg.connect('postgresql://integration_user:integration_password@localhost:5432/postgres')" 2>/dev/null; then
            echo "PostgreSQL ready"
            break
          fi
          sleep 2
        done

        echo "Using partitioncache_integration database for pg_cron compatibility"
        echo "Verifying pg_cron database exists and is configured..."
        python scripts/create_clean_test_database.py --require-pg-cron

        # Set the database name for subsequent steps
        echo "PG_DBNAME=partitioncache_integration" >> $GITHUB_ENV
        echo "DB_NAME=partitioncache_integration" >> $GITHUB_ENV
        echo "PG_QUEUE_DB=partitioncache_integration" >> $GITHUB_ENV
        echo "PG_CRON_DATABASE=partitioncache_integration" >> $GITHUB_ENV

    - name: Run CLI tests
      shell: micromamba-shell {0}
      run: |
        ./scripts/run_integration_tests.sh \
          --ci \
          --pattern test_cli \
          --backend postgresql_array \
          --verbose \
          --no-setup

    - name: Upload CLI test artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: cli-test-results
        path: |
          test-logs/
        retention-days: 7
        if-no-files-found: ignore


  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    needs: [build-postgres-image, integration-tests]
    if: github.event_name == 'push' || github.event_name == 'pull_request'
    
    services:
      postgres:
        image: ghcr.io/mpoppinga/postgres-test-extensions:${{ needs.build-postgres-image.outputs.image-tag }}
        env:
          POSTGRES_USER: integration_user
          POSTGRES_PASSWORD: integration_password
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U integration_user -d postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10
      
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10

    env:
      UNIQUE_DB_NAME: partitioncache_e2e_${{ github.run_id }}
      CACHE_BACKEND: postgresql_array
      PG_HOST: localhost
      PG_PORT: 5432
      PG_USER: integration_user
      PG_PASSWORD: integration_password
      DB_HOST: localhost
      DB_PORT: 5432
      DB_USER: integration_user
      DB_PASSWORD: integration_password
      PG_QUEUE_HOST: localhost
      PG_QUEUE_PORT: 5432
      PG_QUEUE_USER: integration_user
      PG_QUEUE_PASSWORD: integration_password
      QUERY_QUEUE_PROVIDER: postgresql
      
      # Cache backend configuration with E2E-specific prefixes for isolation
      PG_ARRAY_CACHE_TABLE_PREFIX: e2e_array_cache_${{ github.run_id }}
      PG_BIT_CACHE_TABLE_PREFIX: e2e_bit_cache_${{ github.run_id }}
      PG_BIT_CACHE_BITSIZE: 300000
      PG_ROARINGBIT_CACHE_TABLE_PREFIX: e2e_roaring_cache_${{ github.run_id }}
      
      # Queue configuration with E2E-specific prefixes
      PG_QUEUE_TABLE_PREFIX: e2e_queue_${{ github.run_id }}
      INTEGRATION_CACHE_BACKENDS: postgresql_array

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup micromamba
      uses: mamba-org/setup-micromamba@v1
      with:
        micromamba-version: 'latest'
        environment-file: .github/micromamba-env.yml
        init-shell: >-
          bash
        cache-environment: false
        post-cleanup: 'all'

    - name: Install PartitionCache
      shell: micromamba-shell {0}
      run: |
        pip install -e ".[testing,db]"
        pip install pytest-xdist  # Additional for parallel execution

    - name: Create and configure test database
      shell: micromamba-shell {0}
      run: |
        echo "Waiting for PostgreSQL..."
        for i in {1..30}; do
          if python -c "import psycopg; psycopg.connect('postgresql://integration_user:integration_password@localhost:5432/postgres')" 2>/dev/null; then
            echo "PostgreSQL ready"
            break
          fi
          sleep 2
        done

        echo "Creating clean isolated test database: $UNIQUE_DB_NAME"
        python scripts/create_clean_test_database.py

        # Set the database name for subsequent steps
        echo "PG_DBNAME=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "DB_NAME=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "PG_QUEUE_DB=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "PG_CRON_DATABASE=$UNIQUE_DB_NAME" >> $GITHUB_ENV

    - name: Run E2E tests
      shell: micromamba-shell {0}
      run: |
        ./scripts/run_integration_tests.sh \
          --ci \
          --pattern test_end_to_end_workflows \
          --backend postgresql_array \
          --verbose

    - name: Upload E2E artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: e2e-test-results
        path: |
          test-logs/
        retention-days: 7
        if-no-files-found: ignore

  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [build-postgres-image, integration-tests]
    if: github.event_name == 'push' || github.event_name == 'pull_request'
    
    services:
      postgres:
        image: ghcr.io/mpoppinga/postgres-test-extensions:${{ needs.build-postgres-image.outputs.image-tag }}
        env:
          POSTGRES_USER: integration_user
          POSTGRES_PASSWORD: integration_password
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U integration_user -d postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10

    env:
      UNIQUE_DB_NAME: partitioncache_perf_${{ github.run_id }}
      CACHE_BACKEND: postgresql_array
      PG_HOST: localhost
      PG_PORT: 5432
      PG_USER: integration_user
      PG_PASSWORD: integration_password
      DB_HOST: localhost
      DB_PORT: 5432
      DB_USER: integration_user
      DB_PASSWORD: integration_password
      PG_QUEUE_HOST: localhost
      PG_QUEUE_PORT: 5432
      PG_QUEUE_USER: integration_user
      PG_QUEUE_PASSWORD: integration_password
      QUERY_QUEUE_PROVIDER: postgresql
      
      # Cache backend configuration with Performance-specific prefixes for isolation
      PG_ARRAY_CACHE_TABLE_PREFIX: perf_array_cache_${{ github.run_id }}
      PG_BIT_CACHE_TABLE_PREFIX: perf_bit_cache_${{ github.run_id }}
      PG_BIT_CACHE_BITSIZE: 300000
      PG_ROARINGBIT_CACHE_TABLE_PREFIX: perf_roaring_cache_${{ github.run_id }}
      
      # Queue configuration with Performance-specific prefixes  
      PG_QUEUE_TABLE_PREFIX: perf_queue_${{ github.run_id }}

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup micromamba
      uses: mamba-org/setup-micromamba@v1
      with:
        micromamba-version: 'latest'
        environment-file: .github/micromamba-env.yml
        init-shell: >-
          bash
        cache-environment: false
        post-cleanup: 'all'

    - name: Install PartitionCache
      shell: micromamba-shell {0}
      run: |
        pip install -e ".[testing,db]"
        pip install pytest-xdist  # Additional for parallel execution

    - name: Create and configure test database
      shell: micromamba-shell {0}
      run: |
        echo "Waiting for PostgreSQL..."
        for i in {1..30}; do
          if python -c "import psycopg; psycopg.connect('postgresql://integration_user:integration_password@localhost:5432/postgres')" 2>/dev/null; then
            echo "PostgreSQL ready"
            break
          fi
          sleep 2
        done

        echo "Creating clean isolated test database: $UNIQUE_DB_NAME"
        python scripts/create_clean_test_database.py

        # Set the database name for subsequent steps
        echo "PG_DBNAME=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "DB_NAME=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "PG_QUEUE_DB=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "PG_CRON_DATABASE=$UNIQUE_DB_NAME" >> $GITHUB_ENV

    - name: Run performance tests
      shell: micromamba-shell {0}
      run: |
        ./scripts/run_integration_tests.sh \
          --ci \
          --pattern performance \
          --verbose

    - name: Upload performance artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-test-results
        path: |
          performance-results/
        retention-days: 30
        if-no-files-found: ignore

  pg-cron-tests:
    name: pg_cron Integration Tests
    runs-on: ubuntu-latest
    needs: [build-postgres-image, integration-tests]
    if: always() && needs.integration-tests.result == 'success'
    
    services:
      postgres:
        image: ghcr.io/mpoppinga/postgres-test-extensions:${{ needs.build-postgres-image.outputs.image-tag }}
        env:
          POSTGRES_USER: integration_user
          POSTGRES_PASSWORD: integration_password
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U integration_user -d postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10

    env:
      UNIQUE_DB_NAME: partitioncache_integration
      CACHE_BACKEND: postgresql_array
      PG_HOST: localhost
      PG_PORT: 5432
      PG_USER: integration_user
      PG_PASSWORD: integration_password
      DB_HOST: localhost
      DB_PORT: 5432
      DB_USER: integration_user
      DB_PASSWORD: integration_password
      PG_QUEUE_HOST: localhost
      PG_QUEUE_PORT: 5432
      PG_QUEUE_USER: integration_user
      PG_QUEUE_PASSWORD: integration_password
      QUERY_QUEUE_PROVIDER: postgresql
      PG_ARRAY_CACHE_TABLE_PREFIX: ci_array_cache_pgcron
      PG_QUEUE_TABLE_PREFIX: ci_queue_pgcron
      INTEGRATION_CACHE_BACKENDS: postgresql_array

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup micromamba
      uses: mamba-org/setup-micromamba@v1
      with:
        micromamba-version: 'latest'
        environment-file: .github/micromamba-env.yml
        init-shell: >-
          bash
        cache-environment: false
        post-cleanup: 'all'

    - name: Install PartitionCache
      shell: micromamba-shell {0}
      run: |
        pip install -e ".[testing,db]"

    - name: Create and configure test database
      shell: micromamba-shell {0}
      run: |
        echo "Waiting for PostgreSQL..."
        for i in {1..30}; do
          if python -c "import psycopg; psycopg.connect('postgresql://integration_user:integration_password@localhost:5432/postgres')" 2>/dev/null; then
            echo "PostgreSQL ready"
            break
          fi
          sleep 2
        done

        echo "Creating clean isolated test database: $UNIQUE_DB_NAME"
        python scripts/create_clean_test_database.py --require-pg-cron

        # Set the database name for subsequent steps
        echo "PG_DBNAME=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "DB_NAME=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "PG_QUEUE_DB=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "PG_CRON_DATABASE=$UNIQUE_DB_NAME" >> $GITHUB_ENV

    - name: Verify pg_cron extension
      shell: micromamba-shell {0}
      run: |
        python -c "
        import os
        import psycopg
        db_name = os.environ['PG_DBNAME']
        conn_str = f'postgresql://integration_user:integration_password@localhost:5432/{db_name}'
        with psycopg.connect(conn_str, autocommit=True) as conn:
            with conn.cursor() as cur:
                cur.execute('SELECT extname FROM pg_extension WHERE extname = %s', ('pg_cron',))
                if not cur.fetchone():
                    raise Exception('pg_cron extension not found')
                print('pg_cron extension verified')
        "

    - name: Run pg_cron integration tests
      shell: micromamba-shell {0}
      run: |
        python -m pytest \
          tests/integration/test_pg_cron_integration.py \
          tests/integration/test_cross_database_pg_cron.py \
          tests/integration/test_queue_processor.py::TestQueueProcessor::test_schedule_and_execute_task \
          tests/integration/test_queue_processor.py::TestQueueProcessor::test_schedule_job_fast \
          tests/integration/test_error_recovery.py::TestQueueErrorRecovery::test_invalid_cron_job_handling \
          -v -rs

    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: pg-cron-test-results
        path: |
          test-logs/
        retention-days: 7
        if-no-files-found: ignore

  redis-queue-provider-tests:
    name: Redis Queue Provider Tests
    runs-on: ubuntu-latest
    needs: [build-postgres-image]
    if: github.event_name == 'push' || github.event_name == 'pull_request'

    services:
      postgres:
        image: ghcr.io/mpoppinga/postgres-test-extensions:${{ needs.build-postgres-image.outputs.image-tag }}
        env:
          POSTGRES_USER: integration_user
          POSTGRES_PASSWORD: integration_password
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U integration_user -d postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10

    env:
      UNIQUE_DB_NAME: partitioncache_queue_redis_${{ github.run_id }}
      CACHE_BACKEND: redis_set
      INTEGRATION_CACHE_BACKENDS: redis_set
      PG_HOST: localhost
      PG_PORT: 5432
      PG_USER: integration_user
      PG_PASSWORD: integration_password
      DB_HOST: localhost
      DB_PORT: 5432
      DB_USER: integration_user
      DB_PASSWORD: integration_password
      QUERY_QUEUE_PROVIDER: redis
      QUERY_QUEUE_REDIS_DB: 14
      QUERY_QUEUE_REDIS_QUEUE_KEY: ci_queue_redis_${{ github.run_id }}
      REDIS_HOST: localhost
      REDIS_PORT: 6379
      REDIS_CACHE_DB: 0
      REDIS_SET_DB: 0
      REDIS_BIT_DB: 1
      REDIS_BIT_BITSIZE: 300000

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup micromamba
      uses: mamba-org/setup-micromamba@v1
      with:
        micromamba-version: 'latest'
        environment-file: .github/micromamba-env.yml
        init-shell: >-
          bash
        cache-environment: false
        post-cleanup: 'all'

    - name: Install PartitionCache
      shell: micromamba-shell {0}
      run: |
        pip install -e ".[testing,db]"

    - name: Create and configure test database
      shell: micromamba-shell {0}
      run: |
        echo "Waiting for PostgreSQL..."
        for i in {1..30}; do
          if python -c "import psycopg; psycopg.connect('postgresql://integration_user:integration_password@localhost:5432/postgres')" 2>/dev/null; then
            echo "PostgreSQL ready"
            break
          fi
          sleep 2
        done

        echo "Creating clean isolated test database: $UNIQUE_DB_NAME"
        python scripts/create_clean_test_database.py

        # Set the database name for subsequent steps
        echo "PG_DBNAME=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "DB_NAME=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "PG_QUEUE_DB=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "PG_CRON_DATABASE=$UNIQUE_DB_NAME" >> $GITHUB_ENV

    - name: Run Redis queue provider integration test
      shell: micromamba-shell {0}
      run: |
        python -m pytest \
          tests/integration/test_queue_processor.py::TestQueueProcessor::test_concurrent_queue_processing_simulation \
          -v -rs

    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: redis-queue-provider-test-results
        path: |
          test-logs/
        retention-days: 7
        if-no-files-found: ignore

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [integration-tests, cli-tools-tests, e2e-tests, performance-tests, pg-cron-tests, redis-queue-provider-tests]
    if: always()
    
    steps:
    - name: Check results
      run: |
        echo "Integration Test Summary"
        echo "======================="
        
        if [[ "${{ needs.integration-tests.result }}" == "success" ]]; then
          echo "Integration tests: PASSED"
        else
          echo "Integration tests: FAILED"
          exit 1
        fi
        
        if [[ "${{ needs.cli-tools-tests.result }}" == "success" ]]; then
          echo "CLI Tools tests: PASSED"
        else
          echo "CLI Tools tests: FAILED"
          exit 1
        fi

        if [[ "${{ needs.pg-cron-tests.result }}" == "success" ]]; then
          echo "pg_cron tests: PASSED"
        else
          echo "pg_cron tests: FAILED"
          exit 1
        fi

        if [[ "${{ needs.redis-queue-provider-tests.result }}" == "success" ]]; then
          echo "Redis queue provider tests: PASSED"
        else
          echo "Redis queue provider tests: FAILED"
          exit 1
        fi
        
        if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
          if [[ "${{ needs.e2e-tests.result }}" == "success" ]]; then
            echo "E2E tests: PASSED"
          else
            echo "E2E tests: FAILED"
            exit 1
          fi
          
          if [[ "${{ needs.performance-tests.result }}" == "success" ]]; then
            echo "Performance tests: PASSED"
          else
            echo "Performance tests: FAILED"
            exit 1
          fi
        fi
        
        echo "All tests completed successfully"
