name: Integration Tests

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  CI: true
  PYTHON_VERSION: "3.12"

jobs:
  build-postgres-image:
    name: Build PostgreSQL Image with Extensions
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
      actions: write
    outputs:
      image-tag: sha-${{ steps.sha.outputs.short }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Get short SHA
      id: sha
      run: echo "short=${GITHUB_SHA::7}" >> $GITHUB_OUTPUT

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to GitHub Container Registry
      uses: docker/login-action@v3
      with:
        registry: ghcr.io
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ghcr.io/mpoppinga/postgres-test-extensions
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix=sha-
          type=raw,value=latest,enable={{is_default_branch}}

    - name: Build and push PostgreSQL image
      uses: docker/build-push-action@v5
      with:
        context: .github/docker/postgres-cron
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha,scope=${{ github.workflow }}
        cache-to: type=gha,mode=max,scope=${{ github.workflow }}
        platforms: linux/amd64

  integration-tests:
    name: ${{ matrix.name }}
    runs-on: ubuntu-latest
    needs: build-postgres-image
    
    strategy:
      fail-fast: false
      matrix:
        include:
          - name: "PostgreSQL Array Backend"
            pattern: "postgresql_array"
            suffix: "pg_array"
            backend_type: "postgresql_array"
            database_suffix: "array"
            parallel: false
            requires_redis: false
            job_index: 0
          - name: "PostgreSQL Bit Backend"
            pattern: "postgresql_bit"
            suffix: "pg_bit"
            backend_type: "postgresql_bit"
            database_suffix: "bit"
            parallel: false
            requires_redis: false
            job_index: 1
          - name: "PostgreSQL RoaringBit Backend"
            pattern: "postgresql_roaringbit"
            suffix: "pg_roaring"
            backend_type: "postgresql_roaringbit"
            database_suffix: "roaring"
            parallel: false
            requires_redis: false
            job_index: 2
          - name: "Redis Set Backend"
            pattern: "redis_set"
            suffix: "redis_set"
            backend_type: "redis_set"
            database_suffix: "redis_set"
            parallel: false
            requires_redis: true
            job_index: 0
            redis_set_db: 0
          - name: "Redis Bit Backend"
            pattern: "redis_bit"
            suffix: "redis_bit"
            backend_type: "redis_bit"
            database_suffix: "redis_bit"
            parallel: false
            requires_redis: true
            job_index: 1
            redis_bit_db: 1
          - name: "RocksDB Backends"
            pattern: "rocksdb"
            suffix: "rocksdb"
            backend_type: "rocksdb"
            database_suffix: "rocksdb"
            parallel: false
            requires_redis: false
            job_index: 3
          - name: "Queue Processing"
            pattern: "queue"
            suffix: "queue"
            backend_type: "postgresql_array"
            database_suffix: "queue"
            parallel: false
            requires_redis: false
            job_index: 4

    services:
      postgres:
        image: ghcr.io/mpoppinga/postgres-test-extensions:${{ needs.build-postgres-image.outputs.image-tag }}
        env:
          POSTGRES_USER: integration_user
          POSTGRES_PASSWORD: integration_password
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U integration_user -d postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10
      
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10

    env:
      # Define a unique database name for each backend to ensure complete isolation
      UNIQUE_DB_NAME: partitioncache_${{ matrix.database_suffix }}_${{ github.run_id }}
      
      # Set the specific backend type for this job
      CACHE_BACKEND: ${{ matrix.backend_type }}
      
      # PostgreSQL configuration
      PG_HOST: localhost
      PG_PORT: 5432
      PG_USER: integration_user
      PG_PASSWORD: integration_password
      
      # Database configuration
      DB_HOST: localhost
      DB_PORT: 5432
      DB_USER: integration_user
      DB_PASSWORD: integration_password
      
      # Queue configuration
      PG_QUEUE_HOST: localhost
      PG_QUEUE_PORT: 5432
      PG_QUEUE_USER: integration_user
      PG_QUEUE_PASSWORD: integration_password
      QUERY_QUEUE_PROVIDER: postgresql
      
      # Redis configuration - only set for Redis backends
      ${{ matrix.requires_redis && 'REDIS_HOST' || 'REDIS_HOST_DISABLED' }}: ${{ matrix.requires_redis && 'localhost' || 'disabled' }}
      ${{ matrix.requires_redis && 'REDIS_PORT' || 'REDIS_PORT_DISABLED' }}: ${{ matrix.requires_redis && '6379' || 'disabled' }}
      ${{ matrix.redis_set_db != null && 'REDIS_CACHE_DB' || 'REDIS_CACHE_DB_DISABLED' }}: ${{ matrix.redis_set_db || 'disabled' }}
      ${{ matrix.redis_set_db != null && 'REDIS_SET_DB' || 'REDIS_SET_DB_DISABLED' }}: ${{ matrix.redis_set_db || 'disabled' }}
      ${{ matrix.redis_bit_db != null && 'REDIS_BIT_DB' || 'REDIS_BIT_DB_DISABLED' }}: ${{ matrix.redis_bit_db || 'disabled' }}
      ${{ matrix.requires_redis && 'REDIS_BIT_BITSIZE' || 'REDIS_BIT_BITSIZE_DISABLED' }}: ${{ matrix.requires_redis && '300000' || 'disabled' }}
      ${{ matrix.requires_redis && 'REDIS_SET_HOST' || 'REDIS_SET_HOST_DISABLED' }}: ${{ matrix.requires_redis && 'localhost' || 'disabled' }}
      ${{ matrix.requires_redis && 'REDIS_SET_PORT' || 'REDIS_SET_PORT_DISABLED' }}: ${{ matrix.requires_redis && '6379' || 'disabled' }}
      ${{ matrix.requires_redis && 'REDIS_BIT_HOST' || 'REDIS_BIT_HOST_DISABLED' }}: ${{ matrix.requires_redis && 'localhost' || 'disabled' }}
      ${{ matrix.requires_redis && 'REDIS_BIT_PORT' || 'REDIS_BIT_PORT_DISABLED' }}: ${{ matrix.requires_redis && '6379' || 'disabled' }}
      
      # Cache backend configuration with backend-specific prefixes for complete isolation
      PG_ARRAY_CACHE_TABLE_PREFIX: ci_array_${{ matrix.database_suffix }}
      PG_BIT_CACHE_TABLE_PREFIX: ci_bit_${{ matrix.database_suffix }}
      PG_BIT_CACHE_BITSIZE: 300000
      PG_ROARINGBIT_CACHE_TABLE_PREFIX: ci_roaring_${{ matrix.database_suffix }}
      
      # Queue configuration with backend-specific prefixes
      PG_QUEUE_TABLE_PREFIX: ci_queue_${{ matrix.database_suffix }}
      
      # RocksDB configuration - isolated per job to prevent file conflicts
      ROCKSDB_PATH: /tmp/ci_rocksdb_${{ matrix.suffix }}_${{ github.run_id }}
      ROCKSDB_BIT_PATH: /tmp/ci_rocksdb_bit_${{ matrix.suffix }}_${{ github.run_id }}
      ROCKSDB_BIT_BITSIZE: 300000
      ROCKSDB_DICT_PATH: /tmp/ci_rocksdict_${{ matrix.suffix }}_${{ github.run_id }}

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup micromamba
      uses: mamba-org/setup-micromamba@v1
      with:
        micromamba-version: 'latest'
        environment-file: .github/micromamba-env.yml
        init-shell: >-
          bash
        cache-environment: false
        post-cleanup: 'all'

    - name: Install PartitionCache
      shell: micromamba-shell {0}
      run: |
        pip install -e ".[testing,db]"
        pip install pytest-xdist  # Additional for parallel execution

    - name: Create and configure test database
      shell: micromamba-shell {0}
      run: |
        echo "Waiting for PostgreSQL..."
        for i in {1..30}; do
          if python -c "import psycopg; psycopg.connect('postgresql://integration_user:integration_password@localhost:5432/postgres')" 2>/dev/null; then
            echo "PostgreSQL ready"
            break
          fi
          sleep 2
        done

        echo "Creating clean isolated test database: $UNIQUE_DB_NAME"
        python scripts/create_clean_test_database.py

        # Set the database name for subsequent steps
        echo "PG_DBNAME=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "DB_NAME=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "PG_QUEUE_DB=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "PG_CRON_DATABASE=$UNIQUE_DB_NAME" >> $GITHUB_ENV

        if [[ "${{ matrix.requires_redis }}" == "true" ]]; then
          echo "Waiting for Redis..."
          for i in {1..30}; do
            if redis-cli -h localhost -p 6379 ping >/dev/null 2>&1; then
              echo "Redis ready"
              break
            fi
            sleep 2
          done
        fi

    - name: Verify extensions
      shell: micromamba-shell {0}
      run: |
        python -c "
        import os
        import psycopg
        db_name = os.environ['PG_DBNAME']
        conn_str = f'postgresql://integration_user:integration_password@localhost:5432/{db_name}'
        with psycopg.connect(conn_str, autocommit=True) as conn:
            with conn.cursor() as cur:
                # Check for roaringbitmap extension (should be available in all test databases)
                cur.execute('SELECT extname FROM pg_extension WHERE extname = %s', ('roaringbitmap',))
                if cur.fetchone():
                    print('roaringbitmap extension verified')
                else:
                    print('roaringbitmap extension not found (warning)')
                
                # Check for pg_cron extension (only available in designated cron database)
                cur.execute('SELECT extname FROM pg_extension WHERE extname = %s', ('pg_cron',))
                if cur.fetchone():
                    print('pg_cron extension verified (cron-enabled database)')
                else:
                    print('pg_cron extension not found (expected for regular test databases)')
                
                print(f'Database {db_name} extension verification completed')
        "

    - name: Run tests
      shell: micromamba-shell {0}
      run: |
        PYTEST_FLAGS="--ci --pattern ${{ matrix.pattern }} --verbose"
        
        # Exclude pg_cron tests from regular runs (they run in dedicated job)
        PYTEST_FLAGS="$PYTEST_FLAGS --ignore=tests/integration/test_pg_cron_integration.py"
        
        # Exclude CLI tests that require pg_cron from queue processing runs  
        if [[ "${{ matrix.pattern }}" == "queue" ]]; then
          PYTEST_FLAGS="$PYTEST_FLAGS --pytest-args '-k \"not test_postgresql_queue_processor_enable_disable_workflow\"'"
        fi
        
        if [[ "${{ matrix.parallel }}" == "true" ]]; then
          PYTEST_FLAGS="$PYTEST_FLAGS --parallel"
        fi
        
        ./scripts/run_integration_tests.sh $PYTEST_FLAGS --no-setup

    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-${{ matrix.name }}
        path: |
          /tmp/ci_rocksdb*/
          /tmp/ci_rocksdict*/
        retention-days: 7
        if-no-files-found: ignore


  cli-tools-tests:
    name: CLI Tools Tests (pg_cron)
    runs-on: ubuntu-latest
    needs: [build-postgres-image, integration-tests]
    if: github.event_name == 'push' || github.event_name == 'pull_request'
    
    services:
      postgres:
        image: ghcr.io/mpoppinga/postgres-test-extensions:${{ needs.build-postgres-image.outputs.image-tag }}
        env:
          POSTGRES_USER: integration_user
          POSTGRES_PASSWORD: integration_password
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U integration_user -d postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10

    env:
      UNIQUE_DB_NAME: partitioncache_integration
      CACHE_BACKEND: postgresql_array
      PG_HOST: localhost
      PG_PORT: 5432
      PG_USER: integration_user
      PG_PASSWORD: integration_password
      DB_HOST: localhost
      DB_PORT: 5432
      DB_USER: integration_user
      DB_PASSWORD: integration_password
      PG_QUEUE_HOST: localhost
      PG_QUEUE_PORT: 5432
      PG_QUEUE_USER: integration_user
      PG_QUEUE_PASSWORD: integration_password
      QUERY_QUEUE_PROVIDER: postgresql
      PG_ARRAY_CACHE_TABLE_PREFIX: ci_array_cache_cli
      PG_QUEUE_TABLE_PREFIX: ci_queue_cli

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup micromamba
      uses: mamba-org/setup-micromamba@v1
      with:
        micromamba-version: 'latest'
        environment-file: .github/micromamba-env.yml
        init-shell: >-
          bash
        cache-environment: false
        post-cleanup: 'all'

    - name: Install PartitionCache
      shell: micromamba-shell {0}
      run: |
        pip install -e ".[testing,db]"

    - name: Verify and configure pg_cron database
      shell: micromamba-shell {0}
      run: |
        echo "Waiting for PostgreSQL..."
        for i in {1..30}; do
          if python -c "import psycopg; psycopg.connect('postgresql://integration_user:integration_password@localhost:5432/postgres')" 2>/dev/null; then
            echo "PostgreSQL ready"
            break
          fi
          sleep 2
        done

        echo "Using partitioncache_integration database for pg_cron compatibility"
        echo "Verifying pg_cron database exists and is configured..."
        python scripts/create_clean_test_database.py --require-pg-cron

        # Set the database name for subsequent steps
        echo "PG_DBNAME=partitioncache_integration" >> $GITHUB_ENV
        echo "DB_NAME=partitioncache_integration" >> $GITHUB_ENV
        echo "PG_QUEUE_DB=partitioncache_integration" >> $GITHUB_ENV
        echo "PG_CRON_DATABASE=partitioncache_integration" >> $GITHUB_ENV

    - name: Run CLI tests
      shell: micromamba-shell {0}
      run: |
        PYTEST_FLAGS="--ci --pattern test_cli --verbose"
        ./scripts/run_integration_tests.sh $PYTEST_FLAGS --no-setup

    - name: Upload CLI test artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: cli-test-results
        path: |
          test-logs/
        retention-days: 7
        if-no-files-found: ignore


  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    needs: [build-postgres-image, integration-tests]
    if: github.event_name == 'push' || github.event_name == 'pull_request'
    
    services:
      postgres:
        image: ghcr.io/mpoppinga/postgres-test-extensions:${{ needs.build-postgres-image.outputs.image-tag }}
        env:
          POSTGRES_USER: integration_user
          POSTGRES_PASSWORD: integration_password
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U integration_user -d postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10
      
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10

    env:
      UNIQUE_DB_NAME: partitioncache_e2e_${{ github.run_id }}
      CACHE_BACKEND: postgresql_array
      PG_HOST: localhost
      PG_PORT: 5432
      PG_USER: integration_user
      PG_PASSWORD: integration_password
      DB_HOST: localhost
      DB_PORT: 5432
      DB_USER: integration_user
      DB_PASSWORD: integration_password
      PG_QUEUE_HOST: localhost
      PG_QUEUE_PORT: 5432
      PG_QUEUE_USER: integration_user
      PG_QUEUE_PASSWORD: integration_password
      QUERY_QUEUE_PROVIDER: postgresql
      
      # Cache backend configuration with E2E-specific prefixes for isolation
      PG_ARRAY_CACHE_TABLE_PREFIX: e2e_array_cache_${{ github.run_id }}
      PG_BIT_CACHE_TABLE_PREFIX: e2e_bit_cache_${{ github.run_id }}
      PG_BIT_CACHE_BITSIZE: 300000
      PG_ROARINGBIT_CACHE_TABLE_PREFIX: e2e_roaring_cache_${{ github.run_id }}
      
      # Queue configuration with E2E-specific prefixes
      PG_QUEUE_TABLE_PREFIX: e2e_queue_${{ github.run_id }}

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup micromamba
      uses: mamba-org/setup-micromamba@v1
      with:
        micromamba-version: 'latest'
        environment-file: .github/micromamba-env.yml
        init-shell: >-
          bash
        cache-environment: false
        post-cleanup: 'all'

    - name: Install PartitionCache
      shell: micromamba-shell {0}
      run: |
        pip install -e ".[testing,db]"
        pip install pytest-xdist  # Additional for parallel execution

    - name: Create and configure test database
      shell: micromamba-shell {0}
      run: |
        echo "Waiting for PostgreSQL..."
        for i in {1..30}; do
          if python -c "import psycopg; psycopg.connect('postgresql://integration_user:integration_password@localhost:5432/postgres')" 2>/dev/null; then
            echo "PostgreSQL ready"
            break
          fi
          sleep 2
        done

        echo "Creating clean isolated test database: $UNIQUE_DB_NAME"
        python scripts/create_clean_test_database.py

        # Set the database name for subsequent steps
        echo "PG_DBNAME=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "DB_NAME=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "PG_QUEUE_DB=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "PG_CRON_DATABASE=$UNIQUE_DB_NAME" >> $GITHUB_ENV

    - name: Run E2E tests
      shell: micromamba-shell {0}
      run: |
        PYTEST_FLAGS="--ci --pattern test_end_to_end_workflows --verbose"
        ./scripts/run_integration_tests.sh $PYTEST_FLAGS

    - name: Upload E2E artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: e2e-test-results
        path: |
          test-logs/
        retention-days: 7
        if-no-files-found: ignore

  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [build-postgres-image, integration-tests]
    if: github.event_name == 'push' || github.event_name == 'pull_request'
    
    services:
      postgres:
        image: ghcr.io/mpoppinga/postgres-test-extensions:${{ needs.build-postgres-image.outputs.image-tag }}
        env:
          POSTGRES_USER: integration_user
          POSTGRES_PASSWORD: integration_password
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U integration_user -d postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10

    env:
      UNIQUE_DB_NAME: partitioncache_perf_${{ github.run_id }}
      CACHE_BACKEND: postgresql_array
      PG_HOST: localhost
      PG_PORT: 5432
      PG_USER: integration_user
      PG_PASSWORD: integration_password
      DB_HOST: localhost
      DB_PORT: 5432
      DB_USER: integration_user
      DB_PASSWORD: integration_password
      
      # Cache backend configuration with Performance-specific prefixes for isolation
      PG_ARRAY_CACHE_TABLE_PREFIX: perf_array_cache_${{ github.run_id }}
      PG_BIT_CACHE_TABLE_PREFIX: perf_bit_cache_${{ github.run_id }}
      PG_BIT_CACHE_BITSIZE: 300000
      PG_ROARINGBIT_CACHE_TABLE_PREFIX: perf_roaring_cache_${{ github.run_id }}
      
      # Queue configuration with Performance-specific prefixes  
      PG_QUEUE_TABLE_PREFIX: perf_queue_${{ github.run_id }}

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup micromamba
      uses: mamba-org/setup-micromamba@v1
      with:
        micromamba-version: 'latest'
        environment-file: .github/micromamba-env.yml
        init-shell: >-
          bash
        cache-environment: false
        post-cleanup: 'all'

    - name: Install PartitionCache
      shell: micromamba-shell {0}
      run: |
        pip install -e ".[testing,db]"
        pip install pytest-xdist  # Additional for parallel execution

    - name: Create and configure test database
      shell: micromamba-shell {0}
      run: |
        echo "Waiting for PostgreSQL..."
        for i in {1..30}; do
          if python -c "import psycopg; psycopg.connect('postgresql://integration_user:integration_password@localhost:5432/postgres')" 2>/dev/null; then
            echo "PostgreSQL ready"
            break
          fi
          sleep 2
        done

        echo "Creating isolated test database: $UNIQUE_DB_NAME from base template: $BASE_TEMPLATE_DB"
        python -c "
        import os
        import psycopg
        from psycopg import sql
        
        unique_db = os.environ['UNIQUE_DB_NAME']
        base_template = os.environ['BASE_TEMPLATE_DB']
        conn_str = f'postgresql://integration_user:integration_password@localhost:5432/postgres'
        
        with psycopg.connect(conn_str, autocommit=True) as conn:
            with conn.cursor() as cur:
                # Check if database already exists and drop it to ensure clean state
                cur.execute(sql.SQL('SELECT 1 FROM pg_database WHERE datname = %s'), (unique_db,))
                if cur.fetchone():
                    print(f'Dropping existing database: {unique_db}')
                    cur.execute(sql.SQL('DROP DATABASE {}').format(sql.Identifier(unique_db)))
                
                # Create new isolated database from base template
                print(f'Creating database: {unique_db} from template: {base_template}')
                cur.execute(sql.SQL('CREATE DATABASE {} TEMPLATE {}').format(
                    sql.Identifier(unique_db),
                    sql.Identifier(base_template)
                ))
        "

        # Set the database name for subsequent steps
        echo "PG_DBNAME=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "DB_NAME=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "PG_CRON_DATABASE=$UNIQUE_DB_NAME" >> $GITHUB_ENV

    - name: Run performance tests
      shell: micromamba-shell {0}
      run: |
        ./scripts/run_integration_tests.sh \
          --ci \
          --pattern performance \
          --verbose

    - name: Upload performance artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-test-results
        path: |
          performance-results/
        retention-days: 30
        if-no-files-found: ignore

  pg-cron-tests:
    name: pg_cron Integration Tests
    runs-on: ubuntu-latest
    needs: [build-postgres-image, integration-tests]
    if: always() && needs.integration-tests.result == 'success'
    
    services:
      postgres:
        image: ghcr.io/mpoppinga/postgres-test-extensions:${{ needs.build-postgres-image.outputs.image-tag }}
        env:
          POSTGRES_USER: integration_user
          POSTGRES_PASSWORD: integration_password
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U integration_user -d postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10

    env:
      UNIQUE_DB_NAME: partitioncache_integration
      CACHE_BACKEND: postgresql_array
      PG_HOST: localhost
      PG_PORT: 5432
      PG_USER: integration_user
      PG_PASSWORD: integration_password
      DB_HOST: localhost
      DB_PORT: 5432
      DB_USER: integration_user
      DB_PASSWORD: integration_password
      PG_QUEUE_HOST: localhost
      PG_QUEUE_PORT: 5432
      PG_QUEUE_USER: integration_user
      PG_QUEUE_PASSWORD: integration_password
      QUERY_QUEUE_PROVIDER: postgresql
      PG_ARRAY_CACHE_TABLE_PREFIX: ci_array_cache_pgcron
      PG_QUEUE_TABLE_PREFIX: ci_queue_pgcron

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup micromamba
      uses: mamba-org/setup-micromamba@v1
      with:
        micromamba-version: 'latest'
        environment-file: .github/micromamba-env.yml
        init-shell: >-
          bash
        cache-environment: false
        post-cleanup: 'all'

    - name: Install PartitionCache
      shell: micromamba-shell {0}
      run: |
        pip install -e ".[testing,db]"

    - name: Create and configure test database
      shell: micromamba-shell {0}
      run: |
        echo "Waiting for PostgreSQL..."
        for i in {1..30}; do
          if python -c "import psycopg; psycopg.connect('postgresql://integration_user:integration_password@localhost:5432/postgres')" 2>/dev/null; then
            echo "PostgreSQL ready"
            break
          fi
          sleep 2
        done

        echo "Creating clean isolated test database: $UNIQUE_DB_NAME"
        python scripts/create_clean_test_database.py

        # Set the database name for subsequent steps
        echo "PG_DBNAME=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "DB_NAME=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "PG_QUEUE_DB=$UNIQUE_DB_NAME" >> $GITHUB_ENV
        echo "PG_CRON_DATABASE=$UNIQUE_DB_NAME" >> $GITHUB_ENV

    - name: Verify pg_cron extension
      shell: micromamba-shell {0}
      run: |
        python -c "
        import os
        import psycopg
        db_name = os.environ['PG_DBNAME']
        conn_str = f'postgresql://integration_user:integration_password@localhost:5432/{db_name}'
        with psycopg.connect(conn_str, autocommit=True) as conn:
            with conn.cursor() as cur:
                cur.execute('SELECT extname FROM pg_extension WHERE extname = %s', ('pg_cron',))
                if not cur.fetchone():
                    raise Exception('pg_cron extension not found')
                print('pg_cron extension verified')
        "

    - name: Run pg_cron integration tests
      shell: micromamba-shell {0}
      run: |
        # Run only the pg_cron integration tests
        python -m pytest tests/integration/test_pg_cron_integration.py -v

    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: pg-cron-test-results
        path: |
          test-logs/
        retention-days: 7
        if-no-files-found: ignore

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [integration-tests, cli-tools-tests, e2e-tests, performance-tests, pg-cron-tests]
    if: always()
    
    steps:
    - name: Check results
      run: |
        echo "Integration Test Summary"
        echo "======================="
        
        if [[ "${{ needs.integration-tests.result }}" == "success" ]]; then
          echo "Integration tests: PASSED"
        else
          echo "Integration tests: FAILED"
          exit 1
        fi
        
        if [[ "${{ needs.cli-tools-tests.result }}" == "success" ]]; then
          echo "CLI Tools tests: PASSED"
        else
          echo "CLI Tools tests: FAILED"
          exit 1
        fi
        
        if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
          if [[ "${{ needs.e2e-tests.result }}" == "success" ]]; then
            echo "E2E tests: PASSED"
          else
            echo "E2E tests: FAILED"
            exit 1
          fi
          
          if [[ "${{ needs.performance-tests.result }}" == "success" ]]; then
            echo "Performance tests: PASSED"
          else
            echo "Performance tests: FAILED"
            exit 1
          fi
        fi
        
        echo "All tests completed successfully"